{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberbullying model using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df['tweet_text']\n",
    "y = df['cyberbullying_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        In other words #katandandre, your food was cra...\n",
       "1        Why is #aussietv so white? #MKR #theblock #ImA...\n",
       "2        @XochitlSuckkks a classy whore? Or more red ve...\n",
       "3        @Jason_Gio meh. :P  thanks for the heads up, b...\n",
       "4        @RudhoeEnglish This is an ISIS account pretend...\n",
       "                               ...                        \n",
       "47687    Black ppl aren't expected to do anything, depe...\n",
       "47688    Turner did not withhold his disappointment. Tu...\n",
       "47689    I swear to God. This dumb nigger bitch. I have...\n",
       "47690    Yea fuck you RT @therealexel: IF YOURE A NIGGE...\n",
       "47691    Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...\n",
       "Name: tweet_text, Length: 47692, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag) :\n",
    "    if tag.startswith('J') :\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V') :\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N') :\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R') :\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(review) :\n",
    "    global max_len\n",
    "    words = word_tokenize(review)\n",
    "    output_words = []\n",
    "    for word in words :\n",
    "        if word.lower() not in stop_words :\n",
    "            pos = pos_tag([word])\n",
    "            clean_word = lemmatizer.lemmatize(word,pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    max_len = max(max_len, len(output_words))\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In other words #katandandre, your food was crapilicious! #mkr\n",
      "word katandandre food crapilicious mkr\n"
     ]
    }
   ],
   "source": [
    "print(messages[0])\n",
    "messages = [clean_text(message) for message in messages]\n",
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as file:\n",
    "        word_to_vec_map = {}\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        index = 0\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            word_to_index[curr_word] = index\n",
    "            index_to_word[index] = curr_word\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            index += 1\n",
    "    return word_to_index, index_to_word, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = len(X)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        j = 0\n",
    "        for word in sentence_words:\n",
    "            if word in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[word]\n",
    "            j += 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LSTM and CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 505)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 505, 50)      20000000    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 501, 128)     32128       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 505, 64)      29440       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 250, 128)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 64)          0           ['lstm[0][0]']                   \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 128)         0           ['max_pooling1d[0][0]']          \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 192)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          24704       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6)            774         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,087,046\n",
      "Trainable params: 87,046\n",
      "Non-trainable params: 20,000,000\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "597/597 [==============================] - 761s 1s/step - loss: 0.6706 - accuracy: 0.7368 - val_loss: 0.4924 - val_accuracy: 0.7962\n",
      "Epoch 2/10\n",
      "597/597 [==============================] - 803s 1s/step - loss: 0.4723 - accuracy: 0.8060 - val_loss: 0.4646 - val_accuracy: 0.8085\n",
      "Epoch 3/10\n",
      "597/597 [==============================] - 808s 1s/step - loss: 0.4277 - accuracy: 0.8228 - val_loss: 0.4672 - val_accuracy: 0.7997\n",
      "Epoch 4/10\n",
      "597/597 [==============================] - 769s 1s/step - loss: 0.3985 - accuracy: 0.8325 - val_loss: 0.4546 - val_accuracy: 0.8057\n",
      "Epoch 5/10\n",
      "597/597 [==============================] - 749s 1s/step - loss: 0.3722 - accuracy: 0.8422 - val_loss: 0.4559 - val_accuracy: 0.8132\n",
      "Epoch 6/10\n",
      "597/597 [==============================] - 745s 1s/step - loss: 0.3511 - accuracy: 0.8510 - val_loss: 0.4710 - val_accuracy: 0.8172\n",
      "Epoch 7/10\n",
      "597/597 [==============================] - 716s 1s/step - loss: 0.3219 - accuracy: 0.8610 - val_loss: 0.4751 - val_accuracy: 0.8105\n",
      "Epoch 8/10\n",
      "597/597 [==============================] - 701s 1s/step - loss: 0.3085 - accuracy: 0.8667 - val_loss: 0.4885 - val_accuracy: 0.8049\n",
      "Epoch 9/10\n",
      "597/597 [==============================] - 723s 1s/step - loss: 0.2864 - accuracy: 0.8756 - val_loss: 0.4934 - val_accuracy: 0.8099\n",
      "Epoch 10/10\n",
      "597/597 [==============================] - 753s 1s/step - loss: 0.2730 - accuracy: 0.8808 - val_loss: 0.5256 - val_accuracy: 0.8072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x222c756ffa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, concatenate, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_cat = to_categorical(y_encoded)\n",
    "\n",
    "# Split data\n",
    "X = sentences_to_indices(messages, word_to_index, max_len)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "# Embedding layer preparation\n",
    "vocab_len = len(word_to_index)\n",
    "emb_dim = 50\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_len, emb_dim))\n",
    "for word, index in word_to_index.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len,\n",
    "                            output_dim=emb_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False)\n",
    "\n",
    "# Define input\n",
    "input_layer = Input(shape=(max_len,))\n",
    "\n",
    "# Embedding\n",
    "embedded_sequences = embedding_layer(input_layer)\n",
    "\n",
    "# LSTM Branch\n",
    "lstm_branch = LSTM(64, return_sequences=True)(embedded_sequences)\n",
    "lstm_branch = GlobalMaxPooling1D()(lstm_branch)\n",
    "\n",
    "# CNN Branch\n",
    "cnn_branch = Conv1D(filters=128, kernel_size=5, activation='relu')(embedded_sequences)\n",
    "cnn_branch = MaxPooling1D(pool_size=2)(cnn_branch)\n",
    "cnn_branch = GlobalMaxPooling1D()(cnn_branch)\n",
    "\n",
    "# Concatenate both branches\n",
    "merged = concatenate([lstm_branch, cnn_branch])\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "output_layer = Dense(y_cat.shape[1], activation='softmax')(merged)\n",
    "\n",
    "# Define and compile model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 64s 215ms/step - loss: 0.5256 - accuracy: 0.8072\n",
      "Test Accuracy: 80.72%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of LSTM: 92.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"suck it\"\n",
    "text = [clean_text(text)]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sentences_to_indices(text, word_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(text)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_to_index, open('word_to_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
